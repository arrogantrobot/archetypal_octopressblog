<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: bioinformatics | Archetypal]]></title>
  <link href="http://archetyp.al/blog/categories/bioinformatics/atom.xml" rel="self"/>
  <link href="http://archetyp.al/"/>
  <updated>2014-09-14T18:40:25-05:00</updated>
  <id>http://archetyp.al/</id>
  <author>
    <name><![CDATA[Rob Long]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[BioWordCount: An Intro To Bioinformatics on Apache Spark]]></title>
    <link href="http://archetyp.al/blog/2014/09/07/biowordcount-ad-hoc-bioinformatics-on-apache-spark/"/>
    <updated>2014-09-07T18:48:00-05:00</updated>
    <id>http://archetyp.al/blog/2014/09/07/biowordcount-ad-hoc-bioinformatics-on-apache-spark</id>
    <content type="html"><![CDATA[<p>Previously on <a href="http://archetyp.al" title="archetyp.al homepage">archetyp.al</a>, I demonstrated a <a href="http://archetyp.al/blog/2014/05/05/biowordcount-a-brief-introduction-to-hadoop-for-the-bioinformatics-practicioner/" title="previous archetyp.al post on hadoop mapreduce">bioinformatics use for Hadoop MapReduce</a>. The idea was to build on the ubiquitous word count example, but using a problem which is at least somewhat relevant to bioinformatics. So I read in a VCF file and parsed out the reference and the variant bases, and collected an overall count of the mutation spectrum. So here we are, back at it with an <a href="http://spark.apache.org/" title="Apache Spark Homepage">Apache Spark</a> version of the demo. Check out my <a href="http://archetyp.al/spark">listing of useful spark links</a> for a direction to go after this demo.</p>

<p>Why Spark? There are a lot of reasons to go with Spark instead of MapReduce, but for me the most convincing reason is time. In this case the MapReduce solution written in Java takes north of 100 lines of code to set up. Granted, some of that is taken up parsing VCF lines for details which go unused, but 100 is a fair count. Now have a look at the Spark solution:</p>

<p>``` scala BioWordCountSpark.scala https://github.com/plantimals/BioWordCountSpark/blob/master/src/main/scala/al/archetyp/biowordcount/BioWordCountSpark.scala link to github
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf</p>

<p>object BioWordCountSpark {
  def main(args:Array[String]){</p>

<pre><code>val sc = new SparkContext(new SparkConf().setAppName("BioWordCount-Spark"))
val file = sc.textFile(args(0))
val rows = file.filter(!_.startsWith("#"))
val refVar = rows.map(_.split("\t").drop(3).take(2))
val refVarNoIndel = refVar.filter(refvar =&gt; (refvar(0).length() == 1) &amp;&amp; (refvar(1).length() == 1))
val answer = refVarNoIndel.map(data =&gt; (data(0)+" -&gt; "+data(1) -&gt; 1)).reduceByKey(_ + _)
answer.sortByKey().saveAsTextFile("answer")
</code></pre>

<p>  }
}
```</p>

<p>What is this code doing? If you are familiar with Scala, it accomplishes what it looks like it's doing, but via different means. Start out defining a singleton object which contains the main function to be invoked upon execution. The <code>SparkContext</code> object is used to set up the Spark job. Then the input file is loaded from the first command line parameter, <code>arg(0)</code>. All header lines, those which begin with a '#' are filtered.</p>

<p>The <code>refVar = ...</code> line is where I drop the first three columns and take only the next 2 columns, which are the "reference" and "variant" columns, respectively. I feel like there's a better way to select arbitrary columns to keep than the take/drop combination. If you have a suggestion, please leave a comment. Then the collection of ref and variant columns is filtered for length == 1, because we want to remove any indels or structural variants.</p>

<p>Line 12 is really where the magic happens. This is the step that is explicitly a MapReduce operation. The map function is provided a <a href="http://docs.scala-lang.org/tutorials/tour/anonymous-function-syntax.html" title="lambdas or anonymous functions">lambda (or anonymous function)</a>, which is run on each member of the <code>refVarNoIndel</code> collection. This lambda function simply concatenates the two columns, ref and var, into a single string "ref -> var" and then constructs a tuple with that string as the first value, and the number 1 as the second value.</p>

<p>Then, following immediately after the map operation, and on the same line no less, is the <code>reduceByKey</code> function. For those unfamiliar with Scala, the odd looking lambda, <code>(_ + _)</code> is shorthand for <code>(a, b =&gt; (a + b))</code>. This defines how the reduce function combines the values it receives from the map function, it adds each data point to a running total by key, and returns a list of those keys and their respective totals.</p>

<p>Line 13 then sorts the results and pushes them out to the <code>./answer</code> directory.</p>

<p>And that's pretty much everything. There's a an sbt file, which I lifted straight from the <a href="http://spark.apache.org/docs/latest/quick-start.html#standalone-applications" title="spark standalone apps">spark wordcount example</a>. In fact, this whole demo is really an adaptation of the afore mentioned Spark demo, with a slightly different example problem.</p>

<p>In order to set this all up, create a folder to work in. Create a file named <code>build.sbt</code> in the root of your project and paste in the sbt outline from the Spark wordcount example:</p>

<p><code>scala build.sbt
name := "BioWordCount-Spark"
version := "1.0"
scalaVersion := "2.10.4"
libraryDependencies += "org.apache.spark" %% "spark-core" % "1.0.2"
resolvers += "Akka Repository" at "http://repo.akka.io/releases/"
</code></p>

<p>With the sbt file out of the way, let's create the proper spot for the source file <code>mkdir -p src/main/scala/al/archetyp/biowordcount</code>. The source file <code>BioWordCountSpark.scala</code> belongs in the biowordcount directory.</p>

<p>From the root directory of the project, run <code>sbt package</code>. You should see the source file get compiled, and a jar created under <code>./target</code>.</p>

<p>The application should now be compiled and packaged up for usage. Let's look for a tasty VCF to crunch with our new Spark app. I went to <a href="http://www.1000genomes.org/" title="1000 genomes">1000genomes.org</a> and dug up a <a href="ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/ALL.chr22.phase3_shapeit2_mvncall_integrated_v4.20130502.genotypes.vcf.gz" title="1000 genomes VCF chr22 download">VCF containing only variants from human chromosome 22</a>, the shortest autosomal contig. Once unzipped, the file is 10GB, with over 1,000,000 lines. There are over 2000 samples in this VCF, which means each row has more than 2000 columns. I used the following spark-submit command to run the app on this VCF file:</p>

<p><code>bash spark-submit command
time /Users/rob/bin/spark/spark-1.0.2-bin-hadoop2/bin/spark-submit \
--class "BioWordCountSpark" \
--master local[4] \
target/scala-2.10/biowordcount-spark_2.10-1.0.jar \
/Users/rob/Downloads/vcf/ALL.chr22.phase3_shapeit2_mvncall_integrated_v4.20130502.genotypes.vcf
</code></p>

<p>Your paths will vary, especially the location of the spark-submit bin and the vcf file to be counted. The "master" portion of the command directs spark to run this app locally, using as many as 4 cores. If you have a cluster, perhaps on aws, the <code>--master</code> argument should be replaced with the url of the Spark master.</p>

<p>Here follows my resuling count for the 1000genomes VCF:
<code>bash output
cat ./answer/*
(A -&gt; C,31555)
(A -&gt; G,119609)
(A -&gt; T,27133)
(C -&gt; A,46771)
(C -&gt; G,49737)
(C -&gt; T,253419)
(G -&gt; A,254137)
(G -&gt; C,49410)
(G -&gt; T,47227)
(T -&gt; A,26446)
(T -&gt; C,118881)
(T -&gt; G,31131)
</code></p>

<p>When running the app locally on my 2014 Macbook Pro with four cores, it was able to complete within 1 minute and 1 second. The OSX kernel may have cached some of the data, as I did run this command several times, with slight improvements each time. However, for processing a 10GB file with 1 million rows, and over 2000 columns, 1 minute is pretty good. Just to see how this approach stacks up against a python script, I decided to solve the same problem in python. I know that the real benefit to Spark comes when you are running on a multinode cluster, but I was sufficiently impressed with this performance that I wanted to get an idea how long it would take for python to do something similar. Here's my python solution:</p>

<p>``` python biowordcount.py https://github.com/plantimals/BioWordCountSpark/blob/master/src/main/python/biowordcount.py</p>

<h1>!/usr/bin/env python</h1>

<p>import sys</p>

<p>count = {}
with open(sys.argv<a href="http://spark.apache.org/" title="Apache Spark Homepage">1</a>, 'r') as fh:
  for l in fh:</p>

<pre><code>if l[0] == "#":
  continue
cols = l.strip().split("\t")
ref = cols[3]
var = cols[4]
if len(ref) == 1 and len(var) == 1:
  key = "{0} -&gt; {1}".format(ref,var)
  if count.get(key, -1) == -1:
    count[key] = 1
  else:
    count[key] += 1
</code></pre>

<p>for k in count:
  print "({0},{1})".format(k, count[k])
```</p>

<p>When running the above python script on the same vcf, the running time was 1 minute and 40 seconds, reliably. So the speed up is not that amazing. I grabbed larger VCF, this time <a href="ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/release/20130502/ALL.chr1.phase3_shapeit2_mvncall_integrated_v4.20130502.genotypes.vcf.gz" title="100 genomes VCF chr1 download">chromosome 1 from 1000 genomes</a>, which is 61GB when unzipped. The Spark solution took 5 minutes and 50 seconds, while the python solution took 12 minutes and 5 seconds.</p>

<p>Not only is the Spark code faster to write (once you understand the basics), but it actually runs faster. Turn around times on test rus are reduced when the app itself runs faster. It is not unreasonable to assume that code which can be written so much faster and in so many fewer lines will be much easier to read and understand. The density of Scala allows the reader to see and understand larger swaths of the code with in the space of a glance. I'm lookging forward to exploring the Spark ecosystem further.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[BioWordCount: A Brief Introduction to Hadoop For The Bioinformatics Practitioner]]></title>
    <link href="http://archetyp.al/blog/2014/05/05/biowordcount-a-brief-introduction-to-hadoop-for-the-bioinformatics-practicioner/"/>
    <updated>2014-05-05T22:09:00-05:00</updated>
    <id>http://archetyp.al/blog/2014/05/05/biowordcount-a-brief-introduction-to-hadoop-for-the-bioinformatics-practicioner</id>
    <content type="html"><![CDATA[<p>Many people who do bioinformatics (the field seems to have settled on "bioinformatician", but I like "bioinformaticist" better) find themselves dealing with large data sets and long running processes, arranged in myriad pipelines. In our time, this inevitably demands distributed computing. Life innovated during the <a href="https://en.wikipedia.org/wiki/Cambrian_explosion" title="Cambrian Explosion">Cambrian explosion</a> by going from single cells to colonies of cells. Life found a way to distribute and parallelize its processes. In order for us to properly focus our analytical microscopes on life, we imitate life in this strategy and distribute our processes across multiple CPU's.</p>

<p>There are many strategies for distributing computation across multiple cores, processors, or physical computers, but this writeup will only cover the solutions provided by the <a href="https://hadoop.apache.org/" title="Hadoop">Hadoop</a> ecosystem, as packaged by <a href="http://www.cloudera.com/content/cloudera/en/home.html" title="Cloudera">Cloudera</a>. It is the case that as it is widely used now, the Hadoop system of distributed computation and storage does not provide the best solution for long running computations that input or output little data. Even tasks which do use lots of data like sequence alignment, can resist parallelization such that the benefits of Hadoop are negated. Hadoop shines when inputs and or outputs are large in scale, but made of many independent records. Why is this?</p>

<p>Hadoop is a series of services, which when taken together, provide an environment for distributed computation. Each computer in a Hadoop cluster is referred to as a node. A Hadoop environment contains one or more nodes. The majority of the nodes in a multi-node cluster are known as "DataNodes", which store the data and do most of the computation. There are usually at least "NameNodes", which serve to maintain the state of the Hadoop distributed filesystem, but not the contents of any of the files on that filesystem, just the directory structure and the locations of the blocks that make up the files in it.</p>

<h2>The Hadoop Ecosystem</h2>

<p>HDFS is the conceptual core of Hadoop. This is the part that, once understood, can unlock one's view of the whole picture. The key concept is that instead of pushing data to where computations are happening, the computation is pushed to where the data lives. A good example of this is <a href="http://www-03.ibm.com/systems/technicalcomputing/platformcomputing/products/lsf/" title="Platform LSF">Platform LSF</a> or <a href="https://en.wikipedia.org/wiki/Oracle_Grid_Engine" title="Sun Grid Engine">Sun Grid Engine</a>, where the state of the art is to have NFS mounts on your compute nodes, such that when a job lands on that node, it then fetches the required inputs over the network, then does computation on them. Hadoop inverts this order of things by breaking each file up into chunks, default chunk size is 64MB, and distributing these chunks to the DataNodes. The NameNodes track which DataNode has which chunk of which file.</p>

<p>In order to do some computation on those distributed chunks, the MapReduce paradigm enters the picture. When computation needs to be done on a particular file, the required executables (jar files in this case) are sent to those DataNodes hosting at least one chunk of the file. The results of these computations are then gathered in HDFS, and potentially grouped by some key value. This is why large files, containing millions of small, homogeneous records are best for this paradigm. Each chunk is processed, line by line, with the results being aggregated for the whole.</p>

<p>From these two components, HDFS and MapReduce, the other services can be inferred or constructed. There is a distributed column-store database called <a href="https://hbase.apache.org/" title="HBase">HBase</a>, there is a workflow manager called <a href="https://oozie.apache.org/" title="Oozie">Oozie</a>, there is an indexer/search service provided by <a href="https://cwiki.apache.org/confluence/display/solr/SolrCloud" title="SolrCloud">SolrCloud</a>.</p>

<p>It is doubtful that any bioinformatics shop will entirely rely on Hadoop for all its distributed computing needs. It's more and more plausible every day, with projects like <a href="http://bowtie-bio.sourceforge.net/crossbow/index.shtml" title="Crossbow">Crossbow, for bowtie alignment</a> and <a href="http://sourceforge.net/apps/mediawiki/contrail-bio/index.php?title=Contrail" title="Contrail">Contrail for assembly</a>. But in order to really learn how Hadoop works, I suggest an example problem.</p>

<p>I have selected the problem of calculating the mutational spectrum, as it is trivial to understand the mechanics of, is somewhat biologically relevant, and mimics exactly the flow of the classic <a href="https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html" title="WordCount tutorial">Word Count MapReduce example</a>.</p>

<p>In short, <a href="https://en.wikipedia.org/wiki/Mutation_rate#Mutational_spectrum" title="Mutational Spectrum">mutational spectrum</a> is rate at which different types of mutations occur. Transitions are changes from a pyramidine base to another pyramidine, or a purine to another purine. A transversion is the substitution of a purine for a pyramidine, or vice versa.</p>

<p>My goal will be to generate a list of mutations, <code>A -&gt; C</code>, <code>A -&gt; G</code>, etc, with the associated number of occurences in a VCF file. The result will be several lines of text output.</p>

<h2>BioWordCount: The MapReduce Job</h2>

<p>The task of parsing each line of the VCF will be accomplished by a mapreduce job, which will be fed from an input in HDFS, and output an answer to HDFS as well. Begin by creating an empty maven project. There is an excellent tutorial on <a href="http://blog.cloudera.com/blog/2012/08/developing-cdh-applications-with-maven-and-eclipse/" title="maven tutorial">how to generate an empty maven project with the hadoop libraries at Cloudera's website</a>. Follow this, and you will end up with a project ready for our java files. Begin in the <code>src/main/java/al/archetyp/biowordcount</code> directory, replacing the archetyp.al namespace for one of your choosing (preferably the one you used when creating your project).</p>

<p>The first class we create will be the BioWordCount class, which will hold both our mapper and reducer class. For a project of any size, the practice of putting the mapper and reducer in the same class that contains our Main, would be unwise. However, my goal with this demo is to push the complexity down low enough that one can concentrate purely on what mapreduce is and how one might use it.</p>

<p>The mapper class will look like this:</p>

<p>``` java BioWordCount https://github.com/plantimals/biowordcount/blob/master/src/main/java/al/archetyp/biowordcount/BioWordCount.java link to github
public static class Map extends Mapper&lt;LongWritable, Text, Text, IntWritable> {</p>

<pre><code>private final static IntWritable one = new IntWritable(1);
private Text word = new Text();

public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
    if(value.toString().startsWith("#")) return;
    VcfLine vcfLine = new VcfLine(value.toString());
    for(String titv : vcfLine.getTiTv()) {
        word.set(titv);
        context.write(word, one);
    }
}
</code></pre>

<p>}
```
The four type parameters to the Mapper class that we extend are important. The first type, LongWritable, describes the type of the input key, in this case it will be an offset into the file, a line number. The second type is the value of the input. Text is a Hadoop specific type for passing around lines of text from a file. The next two types are the key and value types of the output of the mapper. For the purposes of the biowordcount app, the text  key will be something like "A -> C" and the value, the IntWritable, will always be one, as each line that is processed will only yield a single transition or transversion.</p>

<p>Very simply, for each line of our input file, in this case <a href="http://www.1000genomes.org/wiki/Analysis/Variant%20Call%20Format/vcf-variant-call-format-version-41" title="VCF format">a VCF</a>, the <code>void map(...)</code> function above will be called. The magic of hadoop is that it doesn't matter if that VCF is 100 lines long or 100 million lines long. This class will be distributed to each node where a portion of the file is stored and executed on the lines of the file contained therein.</p>

<p>Each line of the header is skipped by simply returning without writing something to the output context if the line received starts with <code>#</code>. Then we see a helper class I created for this project to contain the parsing of VCF lines. The relevant parts of the VcfLine class are:</p>

<p>``` java VcfLine https://github.com/plantimals/biowordcount/blob/master/src/main/java/al/archetyp/biowordcount/VcfLine.java link to github
public VcfLine(String line) {</p>

<pre><code>StringTokenizer tokenizer = new StringTokenizer(line);
List&lt;String&gt; vcfFields = new ArrayList&lt;String&gt;();
while (tokenizer.hasMoreTokens()) {
    vcfFields.add(tokenizer.nextToken());
}
setChrom(vcfFields.get(0));
setPos(new Long(vcfFields.get(1)));
setRef(vcfFields.get(3));
alts = new ArrayList&lt;String&gt;(Arrays.asList(vcfFields.get(4).split(",")));
isIndel = line.contains("INDEL");
</code></pre>

<p>}</p>

<p>public List<String> getTiTv() {</p>

<pre><code>List&lt;String&gt; answer = new ArrayList&lt;String&gt;();
if (isIndel()) {
    return answer;
}
for (String alt : getAlts()) {
    answer.add(getRef()+" -&gt; "+alt);
}
return answer;
</code></pre>

<p>}
```</p>

<p>The constructor method pulls in a row from a VCF file and parses out each column so we can interrogate the variant in our mapper, finding which <a href="https://en.wikipedia.org/wiki/Transition_(genetics)" title="transition">transition</a> and or <a href="https://en.wikipedia.org/wiki/Transversion" title="transversion">transversion</a> to report. Returning to the Map class, each single nucleotide variation (SNV) is written to the context. This is where the magic happens. We are writing a key/value pair. In this example, the key is the mutation type, "A -> C" for example, and the value will be one. The value is meant to represent a count of the mutations enountered that match the value.</p>

<p>At this point, each SNV is represented as a key value pair in flight between the mapper and the reducer. The <code>void reduce()</code> method of the reducer class will receive a collection containing all those key/value pairs with identical keys. For the purpose of counting the number of like mutations, these collections will be iterated over and summed up, providing a total count of the number of each mutation type.</p>

<p>``` java BioWordCount https://github.com/plantimals/biowordcount/blob/master/src/main/java/al/archetyp/biowordcount/BioWordCount.java link to github</p>

<p>public static class Reduce extends Reducer&lt;Text, IntWritable, Text, IntWritable> {</p>

<pre><code>public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context)
        throws IOException, InterruptedException {
    int sum = 0;
    for (IntWritable val : values) {
        sum += val.get();
    }
    context.write(key, new IntWritable(sum));
}
</code></pre>

<p>}
<code>
The last line of the</code>void reduce()``` function is vital, this is where the aggregated results are written out to the output directory. In order to inspect the complete project with the setup code, the mapper and reduce, along with the VCF parser, <a href="https://github.com/plantimals/biowordcount" title="BioWordCount github">have a look at the github repo</a>.</p>

<h2>Compile and Run</h2>

<p>Once these components are all put together, go to the root of the project folder where the <code>pom.xml</code> file lives. From here, run the command <code>mvn install</code>. This will run for a little while, compiling your java into bytecode and packaging everything into a jar, which should land in the <code>target</code> subdirectory. This jar is what will need to be sent into the Cloudera VM in order to run the job.</p>

<h3>The Cloudera VM</h3>

<p>In order to encapsulate the complexity of installing the Hadoop components on your dev machine, grab the <a href="http://www.cloudera.com/content/support/en/downloads/download-components/download-products.html?productID=F6mO278Rvo&amp;version=2" title="Cloudera VM">Cloudera QuickStart Vm (CDH 4.6)</a>, and download it. I am using Virtual Box, but they also provide vmware and KVM versions of the virtual machine, if those suit your setup better. Cloudera also includes some parts of their own making, like Cloudera Manager, which similarly wraps the complexity of administering the services of hadoop. So all you have to do in order to start using these services, is download the vm and start it up. This step saves hours, if not days, of setup time.</p>

<p>Once you have downloaded the VM and started it, go dig around. It should start up with firefox open to a page that links you to Cloudera Manager. Go to CM and click on the "services" menu on the toolbar at the top, then select "all services". It is possible to stop, start, and restart all the services here. For our purposes, you will need to start, in this order, zookeeper, hdfs, and mapreduce.</p>

<p>I chose to create a shared directory to transfer data and jars between the host system and the guest Cloudera VM. Once the VM is setup and the shared directory created and used, or the jar and vcf data have been scp'd into the guest machine, it is time to run the compiled jar. Once the jar and source data are on the VM, open a terminal or ssh into the VM. The first step to running this example will be to put the source data into HDFS, so that it will be available when the mapreduce job is executed. <code>hadoop fs -mkdir data</code> then <code>hadoop fs -put data/mydata.vcf</code> and <code>hadoop fs -mkdir output</code> should do the trick. Then execute the job by running the command:</p>

<p><code>
hadoop jar BioWordCount.jar al.archetyp.biowordcount.BioWordCount /user/cloudera/data/mydata.vcf /user/cloudera/output
</code></p>

<p>At this point, the jar is sent to those nodes that hold chunks of mydata.vcf. Those nodes then run the various compoenents of the mapreduce process and the final result is deposited in /user/cloudera/output. This can be accessed by the command <code>hadoop fs -cat /user/cloudera/output/part-r-00000</code>, which should produce similar results to those shown below.</p>

<p>Here's what the results look like.</p>

<p><code>
A -&gt; .  78509
A -&gt; C  14752
A -&gt; G  62310
A -&gt; T  610
C -&gt; .  96966
C -&gt; A  14675
C -&gt; G  989
C -&gt; T  62729
G -&gt; .  96803
G -&gt; A  63172
G -&gt; C  795
G -&gt; T  14519
T -&gt; .  79135
T -&gt; A  696
T -&gt; C  62786
T -&gt; G  14491
a -&gt; .  33572
a -&gt; C  6954
a -&gt; G  29519
a -&gt; T  107
c -&gt; .  38822
c -&gt; A  6981
c -&gt; G  172
c -&gt; T  29764
g -&gt; .  38282
g -&gt; A  29399
g -&gt; C  151
g -&gt; T  7010
t -&gt; .  33106
t -&gt; A  113
t -&gt; C  29760
t -&gt; G  6951
</code></p>

<p>The sum total of sites is 954600, which matches exactly the number of non-header lines in the input VCF. While this does not prove the correctness of this approach, it certainly suggests that there are no gross errors where data is added or lost. It's always a good idea to look for ways to check your thinking at each stage of the process. Disproving assumptions can be difficult, but is more likely to result in good work.</p>

<p>I decided to leave in those sites that did not contain variants, <code>X -&gt; .</code>, so that the information would be present in the results. Again, the right course of action always depends upon the context, but I do like to leave in as much data as possible as far into the process as possible, so that the user can decide if it is relevant or not. There are different records for cases where the reference was a capital letter and when the reference was lower case. This is also information I decided to leave in the results. A lowercase letter represents a reference of the genome which lies in the repeatmaker regions, or regions of low complexity.</p>

<p>I used my own data for this project, which was obtained from <a href="https://www.23andme.com/" title="23andme">23andme</a> in the raw format and then converted to VCF via the <a href="http://archetyp.al/blog/2012/08/13/convert-your-23andme-raw-data-into-vcf-format/" title="23andme2vcf archetyp.al">23andme2vcf conversion script described here</a>.</p>

<h2>Conclusion</h2>

<p>Do what thou wilt. As mentioned above, bioinformatics as a field is fairly young, and bioinformatics on Hadoop is even younger. The distributed, mapreduce paradigm is a valuable tool. It is not the only tool, but let's not allow the fact that it doesn't do everything get in the way of letting Hadoop do something, those things that it does well. With the development of more frameworks and tools for Bioinformatics on Hadoop, more adoption will be possible and it will grow.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Convert your 23andme raw data into VCF format]]></title>
    <link href="http://archetyp.al/blog/2012/08/13/convert-your-23andme-raw-data-into-vcf-format/"/>
    <updated>2012-08-13T18:36:00-05:00</updated>
    <id>http://archetyp.al/blog/2012/08/13/convert-your-23andme-raw-data-into-vcf-format</id>
    <content type="html"><![CDATA[<p>A week ago I received my results from <a href="http://www.23andme.com" title="23andme.com">23andme.com</a>. Aside from the obvious points of interest, health risks, heritage, neanderthal composition, etc., I was also interested in getting my own data in raw format. While 23andme does provide a way to download your "raw" data, they are not really providing raw data. One cannot access the image data from the microarray sequencer that they used. What they do provide is formatted as follows:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;h1>rsid  chromosome  position    genotype&lt;/h1>
</span><span class='line'>
</span><span class='line'>&lt;p>rs4477212   1   82154   TT
</span><span class='line'>rs3094315   1   752566  TC
</span><span class='line'>rs3131972   1   752721  AA
</span><span class='line'>rs12124819  1   776546  AC
</span><span class='line'>rs11240777  1   798959  GA
</span><span class='line'>rs6681049   1   800007  CC</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<!-- more -->


<p>Rows that begin with a '#' are header rows, of which, there may be as many as you please. 23andme puts some data in here, like which reference the coordinates are based on. This is an interesting topic as the build being used has just recently changed from hg18 to hg19. If you downloaded your raw data before August 9, 2012, you have hg18, after, and you have hg19. However, someone forgot to update the header to reflect this, so it still reads "build36".</p>

<p>The rsid column is a unique identifier for reference SNP identifier from <a href="http://www.ncbi.nlm.nih.gov/SNP/get_html.cgi?whichHtml=how_to_submit#REFSNP" title="dbSNP">dbSNP</a>. These identifiers were more useful before the completion of the human genome project, as there was no coordinate system capable of resolving the locations of these various SNP's. Now it is possible to address them like you might address a house, with the State or City being analogous to the chromosome and the street address being analogous to the "position". The position is the number of bases from the beginning of the chromosome that a SNP is located at.</p>

<p>The final column is the genotype at the listed address. There are two bases listed because humans have two copies of each chromosome.</p>

<h2>VCF Format</h2>

<p>So this leaves us with a list of addresses. This is well and good, but many bioinformatics applications use a different format, not all that different, called the <a href="http://www.1000genomes.org/wiki/Analysis/Variant%20Call%20Format/vcf-variant-call-format-version-41" title="VCF format">"Variant Call Format"</a>. Specifically, a tool for predicting the biological effects of mutations (bases different than the reference bases), uses the VCF format. It is <a href="http://snpeff.sourceforge.net/" title="snpEff">snpEff</a>, or SNP effect predictor.</p>

<p>In order to facilitate the use of the various and sundry tools that use the VCF format, I have made a tool for converting the 23andme raw format to VCF. It is the <a href="https://github.com/arrogantrobot/23andme2vcf" title="23andme2vcf converter">23andme2vcf converter</a>. In order to use it, follow these steps:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>git clone git://github.com/arrogantrobot/23andme2vcf.git
</span><span class='line'>cd 23andme2vcf
</span><span class='line'>perl 23andme2vcf.pl /path/to/23andme/raw/data.zip /desired/path/to/output.vcf</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>If you do not use git, you may download the tarball from <a href="https://github.com/arrogantrobot/23andme2vcf/tarball/master" title="23andme2vcf converter tarball">github</a>, unpack it, and run line 3 of the above commands.</p>
]]></content>
  </entry>
  
</feed>
